---
phase: 02-data-collection
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/collectors/pull-requests.ts
  - src/collectors/issues.ts
autonomous: true

must_haves:
  truths:
    - "PR authors are extracted from merged and active PRs"
    - "PR reviewers are captured (shows organizational investment)"
    - "Issue authors are extracted"
    - "Issue commenters are captured (shows ongoing engagement)"
    - "All collectors use consistent ContributorActivity aggregation"
  artifacts:
    - path: "src/collectors/pull-requests.ts"
      provides: "PR author and reviewer extraction"
      exports: ["PullRequestCollector"]
    - path: "src/collectors/issues.ts"
      provides: "Issue author and commenter extraction"
      exports: ["IssueCollector"]
  key_links:
    - from: "src/collectors/pull-requests.ts"
      to: "src/collectors/types.ts"
      via: "implements Collector interface"
      pattern: "implements Collector"
    - from: "src/collectors/issues.ts"
      to: "src/collectors/types.ts"
      via: "implements Collector interface"
      pattern: "implements Collector"
    - from: "src/collectors/pull-requests.ts"
      to: "src/parsers/pagination.ts"
      via: "page-based pagination"
      pattern: "extractNextPage"
    - from: "src/collectors/issues.ts"
      to: "src/parsers/pagination.ts"
      via: "page-based pagination"
      pattern: "extractNextPage"
---

<objective>
Create collectors for pull requests and issues, extracting authors, reviewers, and commenters with pagination support.

Purpose: Capture the two remaining contributor sources (PRs and issues) to complete the data collection layer.
Output: Working PR and issue collectors that extract all participant types with page-based pagination.
</objective>

<execution_context>
@/Users/jesse/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jesse/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-collection/02-CONTEXT.md
@.planning/phases/02-data-collection/02-RESEARCH.md
@.planning/phases/02-data-collection/02-01-SUMMARY.md
@src/collectors/types.ts
@src/collectors/commits.ts
@src/parsers/pagination.ts
@src/filters/bots.ts
@src/scraper/client.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pull request collector</name>
  <files>
    src/collectors/pull-requests.ts
  </files>
  <action>
Create `src/collectors/pull-requests.ts`:
- Import cheerio, GitHubClient, types, filters, date parsers
- PullRequestCollector class implementing Collector interface
- Constructor takes GitHubClient instance

getStartUrl(owner: string, repo: string, since?: Date): string
- Base: `https://github.com/${owner}/${repo}/pulls`
- Query: `?q=is:pr+is:merged` for merged PRs (start with merged, most valuable)
- If since provided, add `+merged:>${formatDate(since)}` (ISO date YYYY-MM-DD)
- Return full URL

collectPage(url: string, since?: Date): Promise<CollectorResult>
- Fetch HTML via client.fetch(url)
- Load into cheerio
- Parse PR list items (try selectors like '.Box-row' or similar PR list elements)
- For each PR:
  - Extract PR author username from author link
  - Extract PR date (opened or merged date from relative-time)
  - If since provided and date < since, skip
  - If isBot(username), skip
  - Increment prsAuthored for author

Reviewer extraction strategy:
- Option A: If reviewer info visible in list HTML (check for reviewer avatars/links), extract directly
- Option B: If not visible, we need to fetch PR detail page for each PR
- For now, implement Option B as fallback: For each PR link found, fetch the detail page and extract reviewers from the review section
- Store PR URLs for detail fetching, or fetch inline if context budget allows
- Reviewers are typically in the right sidebar or in review timeline
- Increment prsReviewed for each unique reviewer

Extract nextPage using extractNextPage($, 'prs')
Return { contributors, nextPage, itemsProcessed }

Note: Fetching each PR detail page for reviewers is expensive. Consider caching aggressively. The GitHubClient already caches, so this should be efficient on subsequent runs.

Also collect open PRs with recent activity:
- Add a second pass or separate method: getOpenPRsUrl() returning `?q=is:pr+is:open+updated:>${thirtyDaysAgo}`
- Flag closed-unmerged PRs (from is:pr+is:closed+is:unmerged) with lower weight marker
  </action>
  <verify>
```bash
npm run build
```
Build succeeds. PullRequestCollector is exported.
  </verify>
  <done>
PR collector extracts authors from merged/active PRs and reviewers from PR detail pages. Page-based pagination handles large PR lists.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create issue collector</name>
  <files>
    src/collectors/issues.ts
  </files>
  <action>
Create `src/collectors/issues.ts`:
- Import cheerio, GitHubClient, types, filters, date parsers
- IssueCollector class implementing Collector interface
- Constructor takes GitHubClient instance

getStartUrl(owner: string, repo: string, since?: Date): string
- Base: `https://github.com/${owner}/${repo}/issues`
- Query: `?q=is:issue`
- If since provided, add `+created:>${formatDate(since)}` (ISO date)
- Return full URL

collectPage(url: string, since?: Date): Promise<CollectorResult>
- Fetch HTML via client.fetch(url)
- Load into cheerio
- Parse issue list items
- For each issue:
  - Extract author username
  - Extract created date
  - If since provided and date < since, skip
  - If isBot(username), skip
  - Increment issuesAuthored for author
  - Store issue URL for comment extraction

Commenter extraction:
- For each issue, fetch detail page
- Parse comments section (look for comment elements with author links)
- For each comment:
  - Extract commenter username
  - Skip if isBot(commenter)
  - Increment issuesCommented for commenter
- Handle comment pagination if issue has many comments (GitHub paginates long threads)

Extract nextPage using extractNextPage($, 'issues')
Return { contributors, nextPage, itemsProcessed }

Performance note: Fetching every issue detail page is expensive for repos with thousands of issues. Consider:
- Only fetching issues within time window
- Limiting to issues with comments (if detectable from list)
- The cache will help on repeated runs
  </action>
  <verify>
```bash
npm run build
```
Build succeeds. IssueCollector is exported.
  </verify>
  <done>
Issue collector extracts authors and commenters. Detail pages are fetched for comment extraction. Page-based pagination handles large issue lists.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `npm run build` succeeds
2. New files exist: src/collectors/pull-requests.ts, src/collectors/issues.ts
3. Both collectors implement the Collector interface from types.ts
4. Both use pagination extraction from parsers/pagination.ts
5. Both filter bots using filters/bots.ts
</verification>

<success_criteria>
- PullRequestCollector extracts PR authors from merged and open-active PRs
- PullRequestCollector extracts reviewers (via detail page fetch)
- IssueCollector extracts issue authors
- IssueCollector extracts commenters (via detail page fetch)
- Both collectors use page-based pagination (?page=N)
- Both respect time filtering (--since)
- Both filter bot accounts
- Activity counts properly increment prsAuthored, prsReviewed, issuesAuthored, issuesCommented
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-collection/02-02-SUMMARY.md`
</output>
